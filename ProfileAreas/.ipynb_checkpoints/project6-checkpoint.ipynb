{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f1cbba",
   "metadata": {
    "id": "f9f1cbba"
   },
   "source": [
    "# Introduction to Profile Areas - Image Analysis\n",
    "## Project 6\n",
    "\n",
    "This is your baseline code to tweak and play around with in order to increase the IoU.\n",
    "It is customised from the following repository: https://github.com/GeorgeBatch/kvasir-seg/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DGkgfAJJdi35",
   "metadata": {
    "id": "DGkgfAJJdi35"
   },
   "source": [
    "If you are using Google Colab, please remember that you have to switch from CPU to GPU (Runtime --> change runtime type)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41463e4",
   "metadata": {
    "id": "a41463e4"
   },
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc0c54",
   "metadata": {
    "id": "f2bc0c54"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import imageio as iio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089c460",
   "metadata": {
    "id": "9089c460"
   },
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owSIZFeZRerk",
   "metadata": {
    "id": "owSIZFeZRerk"
   },
   "source": [
    "The input images are RGB and the segmentation masks are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c92a1",
   "metadata": {
    "id": "a40c92a1"
   },
   "outputs": [],
   "source": [
    "image_channels = 3\n",
    "mask_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U2wjADYfQ6HM",
   "metadata": {
    "id": "U2wjADYfQ6HM"
   },
   "source": [
    "Parameters in the following block can be adjusted in order to achieve better results, e.g. you could change the batch size or try out different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xqPWuN9ERUjq",
   "metadata": {
    "id": "xqPWuN9ERUjq"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-3\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d1de3",
   "metadata": {
    "id": "e90d1de3"
   },
   "source": [
    "### Loading data set\n",
    "\n",
    "We are working with the \"Kvasir SEG\" data set (Segmented Polyp Dataset for Computer Aided Gastrointestinal Disease Detection). You can download it at https://datasets.simula.no/kvasir-seg/. There are two separate folders containing the images and segmentation masks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51f61b",
   "metadata": {
    "id": "df51f61b"
   },
   "outputs": [],
   "source": [
    "path_images = '<your_path_to_data_set>/Kvasir-SEG/images'\n",
    "\n",
    "path_masks = '<your_path_to_data_set>/Kvasir-SEG/masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12a188",
   "metadata": {
    "id": "df12a188"
   },
   "source": [
    "In addition, to ensure that everyone uses the same training/validation/testing split, please download the files 'train.txt' and 'val.txt' from https://github.com/GeorgeBatch/kvasir-seg/tree/main/train-val-split and save them in the data set folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc88a15",
   "metadata": {
    "id": "2bc88a15"
   },
   "outputs": [],
   "source": [
    "train_ids_txt = '<your_path_to_data_set>/Kvasir-SEG/train.txt'\n",
    "\n",
    "valid_ids_txt = '<your_path_to_data_set>/Kvasir-SEG/val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c187bc7",
   "metadata": {
    "id": "8c187bc7"
   },
   "outputs": [],
   "source": [
    "with open(train_ids_txt, 'r') as f:\n",
    "    ids_train = [l.strip()+'.jpg' for l in f]\n",
    "\n",
    "with open(valid_ids_txt, 'r') as f:\n",
    "    ids_val_test = [l.strip()+'.jpg' for l in f]\n",
    "\n",
    "ids_val = ids_val_test[:60]\n",
    "ids_test = ids_val_test[60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LHxUbWrPNa98",
   "metadata": {
    "id": "LHxUbWrPNa98"
   },
   "source": [
    "### Data set class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbb426",
   "metadata": {
    "id": "33bbb426"
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, ids, path_images, path_masks, transforms):\n",
    "        self.ids = ids\n",
    "        self.path_images = path_images\n",
    "        self.path_masks = path_masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_img = os.path.join(self.path_images, self.ids[index])\n",
    "        path_mask = os.path.join(self.path_masks, self.ids[index])\n",
    "\n",
    "        # Load and normalise image\n",
    "        img = iio.v3.imread(path_img) / 255\n",
    "        # Load, normalise and binarise mask\n",
    "        mask = iio.v3.imread(path_mask)[:, :, 0] / 255\n",
    "        mask = mask.round()\n",
    "\n",
    "        # Make sure the dimensions are suitable for PyTorch\n",
    "        img = torch.FloatTensor(np.transpose(img, [2, 0 ,1]))\n",
    "        mask = torch.FloatTensor(mask).unsqueeze(0)\n",
    "\n",
    "        # Ensure that data augmentation is identical for image and mask\n",
    "        sample = torch.cat((img, mask), 0)\n",
    "        sample = self.transforms(sample)\n",
    "        img = sample[:img.shape[0], ...]\n",
    "        mask = sample[img.shape[0]:, ...]\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dncrKkRAPcmj",
   "metadata": {
    "id": "dncrKkRAPcmj"
   },
   "source": [
    "### Data augmentation\n",
    "Here, you could also add different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21e7f4",
   "metadata": {
    "id": "7b21e7f4"
   },
   "outputs": [],
   "source": [
    "SIZE = (256, 256)\n",
    "INTERPOLATION_MODE = transforms.InterpolationMode.NEAREST\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.Resize(SIZE, interpolation=INTERPOLATION_MODE),\n",
    "                           transforms.RandomHorizontalFlip(0.5)\n",
    "                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                          transforms.Resize(SIZE, interpolation=INTERPOLATION_MODE),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnAbO9VhQEwW",
   "metadata": {
    "id": "qnAbO9VhQEwW"
   },
   "source": [
    "### Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5825f",
   "metadata": {
    "id": "80e5825f"
   },
   "outputs": [],
   "source": [
    "custom_dataset_train = DataSet(ids_train, path_images, path_masks, transforms=train_transforms)\n",
    "custom_dataset_val = DataSet(ids_val, path_images, path_masks, transforms=val_transforms)\n",
    "custom_dataset_test = DataSet(ids_test, path_images, path_masks, transforms=val_transforms)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "        custom_dataset_train, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "        custom_dataset_val, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "        custom_dataset_test, batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9c16b",
   "metadata": {
    "id": "b2a9c16b"
   },
   "source": [
    "### IoU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada6296",
   "metadata": {
    "id": "7ada6296"
   },
   "outputs": [],
   "source": [
    "def iou_eval(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    outputs = outputs > 0.5\n",
    "\n",
    "    outputs = outputs.squeeze(1).byte()\n",
    "    labels = labels.squeeze(1).byte()\n",
    "\n",
    "    # Smooth in order to avoid division 0/0\n",
    "    SMOOTH = 1e-8\n",
    "    intersection = (outputs & labels).float().sum((1, 2))\n",
    "    union = (outputs | labels).float().sum((1, 2))\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "\n",
    "    return iou.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a0466",
   "metadata": {
    "id": "198a0466"
   },
   "source": [
    "### Constructing the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k8IzfCfnhP8u",
   "metadata": {
    "id": "k8IzfCfnhP8u"
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "\n",
    "    def conv_block(self, channel_in, channel_out):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(channel_out),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(channel_out, channel_out, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(channel_out),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, bilinear=None):\n",
    "        super(UNet, self).__init__()\n",
    "        self.channel_in = channel_in\n",
    "        self.channel_out = channel_out\n",
    "\n",
    "        # initial convolutional block\n",
    "        self.initial = self.conv_block(channel_in, 64)\n",
    "\n",
    "        # encoder layers\n",
    "        self.down0 = self.conv_block(64, 128)\n",
    "        self.down1 = self.conv_block(128, 256)\n",
    "        self.down2 = self.conv_block(256, 512)\n",
    "        self.down3 = self.conv_block(512, 1024)\n",
    "\n",
    "        # decoder layers\n",
    "        self.up0_0 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up0_1 = self.conv_block(1024, 512)\n",
    "        self.up1_0 = torch.nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up1_1 = self.conv_block(512, 256)\n",
    "        self.up2_0 = torch.nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up2_1 = self.conv_block(256, 128)\n",
    "        self.up3_0 = torch.nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up3_1 = self.conv_block(128, 64)\n",
    "\n",
    "        # final layer before output\n",
    "        self.final = torch.nn.Conv2d(64, channel_out, kernel_size=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_in = self.initial(x)\n",
    "        enc0 = self.down0(torch.nn.MaxPool2d(2)(x_in))\n",
    "        enc1 = self.down1(torch.nn.MaxPool2d(2)(enc0))\n",
    "        enc2 = self.down2(torch.nn.MaxPool2d(2)(enc1))\n",
    "        enc3 = self.down3(torch.nn.MaxPool2d(2)(enc2))\n",
    "\n",
    "        dec0 = self.up0_0(enc3)\n",
    "        diff0 = torch.FloatTensor(list(enc2.size())[2:]) - torch.FloatTensor(list(dec0.shape))[2:]\n",
    "        dec0 = torch.nn.functional.pad(dec0, (int((diff0/2).floor()[0]), int((diff0/2).ceil()[0]), int((diff0/2).floor()[1]), int((diff0/2).ceil()[1])))\n",
    "        dec0 = self.up0_1(torch.cat((enc2, dec0), dim=1))\n",
    "\n",
    "        dec1 = self.up1_0(dec0)\n",
    "        diff1 = torch.FloatTensor(list(enc1.size())[2:]) - torch.FloatTensor(list(dec1.shape))[2:]\n",
    "        dec1 = torch.nn.functional.pad(dec1, (int((diff1/2).floor()[0]), int((diff1/2).ceil()[0]), int((diff1/2).floor()[1]), int((diff1/2).ceil()[1])))\n",
    "        dec1 = self.up1_1(torch.cat((enc1, dec1), dim=1))\n",
    "\n",
    "        dec2 = self.up2_0(dec1)\n",
    "        diff2 = torch.FloatTensor(list(enc0.size())[2:]) - torch.FloatTensor(list(dec2.shape))[2:]\n",
    "        dec2 = torch.nn.functional.pad(dec2, (int((diff2/2).floor()[0]), int((diff2/2).ceil()[0]), int((diff2/2).floor()[1]), int((diff2/2).ceil()[1])))\n",
    "        dec2 = self.up2_1(torch.cat((enc0, dec2), dim=1))\n",
    "\n",
    "        dec3 = self.up3_0(dec2)\n",
    "        diff3 = torch.FloatTensor(list(x.size())[2:]) - torch.FloatTensor(list(dec3.shape))[2:]\n",
    "        dec3 = torch.nn.functional.pad(dec3, (int((diff3/2).floor()[0]), int((diff3/2).ceil()[0]), int((diff3/2).floor()[1]), int((diff3/2).ceil()[1])))\n",
    "        dec3 = self.up3_1(torch.cat((x_in, dec3), dim=1))\n",
    "\n",
    "        x_out = self.final(dec3)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611b783",
   "metadata": {
    "id": "b611b783"
   },
   "outputs": [],
   "source": [
    "model = UNet(channel_in=image_channels, channel_out=mask_channels)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DqYX6-rFl_Qt",
   "metadata": {
    "id": "DqYX6-rFl_Qt"
   },
   "source": [
    "Another option would be to play around with the optimiser and its inputs and change the learning rate or add a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uu6tZrUBmCwC",
   "metadata": {
    "id": "uu6tZrUBmCwC"
   },
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54d413",
   "metadata": {
    "id": "3e54d413"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O2MIiPDCknUM",
   "metadata": {
    "id": "O2MIiPDCknUM"
   },
   "outputs": [],
   "source": [
    "def train_val_one_epoch(model, optimiser, criterion, dataloader, epoch, device, train_mode):\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "\n",
    "    for i, (imgs, masks) in enumerate(dataloader):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            prediction = model(imgs)\n",
    "            loss = criterion(prediction, masks)\n",
    "\n",
    "            if train_mode:\n",
    "                optimiser.zero_grad()\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_iou += iou_eval(prediction, masks).item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "\n",
    "    mode = 'Training' if train_mode else 'Validation'\n",
    "    print(f'Epoch: {epoch+1} of {num_epochs}, {mode} Avg Epoch Loss: {avg_loss:.6f}, Avg Epoch IoU: {avg_iou:.6f}')\n",
    "\n",
    "    return avg_loss, avg_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d236aba",
   "metadata": {
    "id": "1d236aba"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_iou = []\n",
    "val_iou = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_avg_train_loss, epoch_avg_train_iou = train_val_one_epoch(\n",
    "        model, optimiser, loss, dataloader_train, epoch, device='cuda', train_mode=True)\n",
    "    epoch_avg_val_loss, epoch_avg_val_iou = train_val_one_epoch(\n",
    "        model, optimiser, loss, dataloader_val, epoch, device='cuda', train_mode=False)\n",
    "\n",
    "    train_losses.append(epoch_avg_train_loss)\n",
    "    val_losses.append(epoch_avg_val_loss)\n",
    "    train_iou.append(epoch_avg_train_iou)\n",
    "    val_iou.append(epoch_avg_val_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mqqq8wC4SfEB",
   "metadata": {
    "id": "Mqqq8wC4SfEB"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UY_ovTjRSqPr",
   "metadata": {
    "id": "UY_ovTjRSqPr"
   },
   "outputs": [],
   "source": [
    "# Please add code for the quantitative and qualitative testing here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
